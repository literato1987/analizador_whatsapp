# %% [markdown]
# # ğŸ¤– WhatsApp Chat Style Mimicker
# 
# Este script analiza un chat de WhatsApp y puede imitar cÃ³mo habla cada persona.
# 
# ## ğŸ“ Antes de empezar:
# 1. Exporta un chat de WhatsApp (sin medios)
# 2. Pon el archivo .txt en esta carpeta
# 3. Instala las dependencias necesarias

# %% [markdown]
# ## ğŸ“š Importar lo necesario

# %%
import os
import re
import pandas as pd
from datetime import datetime
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch

print("ğŸ”§ Configurando el modelo...")

# Usar un modelo mÃ¡s ligero y gratuito
model_name = "facebook/blenderbot-400M-distill"
try:
    # Inicializar el generador de texto
    generator = pipeline(
        "text-generation",
        model=model_name,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    print("âœ… Modelo cargado correctamente")
except Exception as e:
    print(f"âŒ Error al cargar el modelo: {str(e)}")
    print("Intentando descargar el modelo primero...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
        print("âœ… Modelo descargado y cargado correctamente")
    except Exception as e:
        raise Exception(f"âŒ No se pudo cargar el modelo: {str(e)}")

# %% [markdown]
# ## ğŸ“‚ Buscar archivos de chat

# %%
# Ver quÃ© archivos de chat hay disponibles
chat_files = [f for f in os.listdir() if f.endswith('.txt')]

print("ğŸ“ Archivos encontrados:")
for i, file in enumerate(chat_files, 1):
    print(f"{i}. {file}")

if not chat_files:
    print("âŒ No se encontraron archivos .txt")
    raise Exception("Necesitas exportar un chat de WhatsApp primero")

# Elegir un archivo
file_number = int(input("\nElige el nÃºmero del archivo a analizar: "))
selected_file = chat_files[file_number - 1]
print(f"Archivo seleccionado: {selected_file}")

# %% [markdown]
# ## ğŸ“– Leer y procesar el chat

# %%
# Leer el archivo y mostrar las primeras lÃ­neas para debug
with open(selected_file, 'r', encoding='utf-8') as file:
    content = file.read()
    
print("Primeras 5 lÃ­neas del archivo:")
print("\n".join(content.split('\n')[:5]))
print("\nTotal de lÃ­neas:", len(content.split('\n')))

# Intentar diferentes patrones de regex para WhatsApp
patterns = [
    # PatrÃ³n 1: Formato tÃ­pico de WhatsApp
    r'(\d{1,2}/\d{1,2}/\d{2,4},\s\d{1,2}:\d{2})\s-\s([^:]+):\s(.+)',
    
    # PatrÃ³n 2: Con AM/PM
    r'(\d{1,2}/\d{1,2}/\d{2,4},\s\d{1,2}:\d{2}\s[APMapm]{2})\s-\s([^:]+):\s(.+)',
    
    # PatrÃ³n 3: Formato alternativo
    r'\[(\d{1,2}/\d{1,2}/\d{2,4}\s\d{1,2}:\d{2}:\d{2})\]\s([^:]+):\s(.+)'
]

matches = []
used_pattern = None

for pattern in patterns:
    matches = re.findall(pattern, content, re.MULTILINE)
    if matches:
        used_pattern = pattern
        print(f"\nâœ… PatrÃ³n encontrado: {pattern}")
        print(f"Mensajes encontrados: {len(matches)}")
        break

if not matches:
    print("\nâŒ No se pudo encontrar un patrÃ³n vÃ¡lido en el archivo")
    print("Por favor, verifica que el archivo es una exportaciÃ³n de WhatsApp")
    raise Exception("Formato de archivo no reconocido")

# Crear DataFrame
df = pd.DataFrame(matches, columns=['datetime', 'sender', 'message'])
print("\nEstructura del DataFrame:")
print(df.head())

# Ver miembros del chat
members = df['sender'].unique().tolist()
print(f"\nğŸ‘¥ Miembros encontrados ({len(members)}):")
for i, member in enumerate(members, 1):
    print(f"{i}. {member}")

if not members:
    print("âŒ No se encontraron miembros en el chat")
    raise Exception("No se pudieron extraer los miembros del chat")

# %% [markdown]
# ## ğŸ¯ Seleccionar miembro y analizar su estilo

# %%
# Elegir miembro
member_number = int(input("\nElige el nÃºmero del miembro a imitar: "))
selected_member = members[member_number - 1]

# Obtener mensajes del miembro
member_messages = df[df['sender'] == selected_member]['message'].tolist()
sample_messages = member_messages[-20:] if len(member_messages) > 20 else member_messages

print(f"\nAnalizando los Ãºltimos {len(sample_messages)} mensajes de {selected_member}")
print("\nEjemplos de mensajes:")
for i, msg in enumerate(sample_messages[:3], 1):
    print(f"{i}. {msg}")

# %% [markdown]
# ## ğŸ’¬ Generar mensaje en su estilo

# %%
# Preparar el prompt
prompt = input("\nEscribe el mensaje que quieres generar: ")

# Crear el contexto con los mensajes de ejemplo
context = "\n".join([
    "Ejemplos de cÃ³mo habla esta persona:",
    *[f"- {msg}" for msg in sample_messages[-5:]],  # Usar los Ãºltimos 5 mensajes como ejemplo
    "\nAhora, responde en el mismo estilo a este mensaje:",
    prompt
])

try:
    # Generar respuesta
    response = generator(
        context,
        max_length=150,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True
    )

    # Extraer la respuesta generada
    generated_text = response[0]['generated_text']
    # Limpiar la respuesta (quedarnos solo con la Ãºltima parte despuÃ©s del prompt)
    final_response = generated_text.split(prompt)[-1].strip()

    print(f"\nğŸ’¬ Mensaje generado en el estilo de {selected_member}:")
    print(final_response)
except Exception as e:
    print(f"âŒ Error al generar el mensaje: {str(e)}")
    print("Intenta con un prompt mÃ¡s corto o diferente")

# %% [markdown]
# ## ğŸ­ Generar mensaje tÃ­pico de cada miembro

# %%
print("ğŸ”„ Verificando el modelo...")

# Asegurarnos de que el modelo estÃ¡ cargado y que tenemos datos
if 'generator' not in locals():
    print("âš™ï¸ Cargando el modelo...")
    try:
        # Usar un modelo mÃ¡s ligero y en espaÃ±ol
        model_name = "PlanTL-GOB-ES/gpt2-base-bne"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
        print("âœ… Modelo cargado correctamente")
    except Exception as e:
        print(f"âŒ Error al cargar el modelo: {str(e)}")
        print("Intentando un modelo alternativo...")
        try:
            # Intentar con un modelo mÃ¡s pequeÃ±o
            model_name = "bertin-project/bertin-gpt-j-6B-text"
            generator = pipeline(
                "text-generation",
                model=model_name,
                device="cuda" if torch.cuda.is_available() else "cpu"
            )
            print("âœ… Modelo alternativo cargado correctamente")
        except Exception as e:
            print(f"âŒ Error al cargar el modelo alternativo: {str(e)}")
            raise Exception("No se pudo cargar ningÃºn modelo de generaciÃ³n de texto")

# Verificar que tengamos datos de chat y miembros
if not members or df is None:
    print("âŒ Error: No hay datos de chat o miembros cargados. Por favor, analiza un archivo de chat primero.")
    raise Exception("No hay datos de chat cargados")

print("\nğŸ’¬ Generando mensajes tÃ­picos de cada miembro...")

# Prompts variados para obtener diferentes tipos de mensajes
prompts = [
    # Relacionados con beber/fiesta
    "Â¿Alguien se apunta a unas cervezas?",
    "Â¿Quedamos en el bar de siempre?",
    "Â¿QuiÃ©n viene de cubatas?",
    "Â¿Hacemos una previa antes de salir?",
    "Â¿Alguien tiene resaca hoy? ğŸ¤¢",
    "Â¿DÃ³nde nos tomamos la Ãºltima?",
    
    # Relacionados con mÃºsica/tocar
    "Â¿Ensayamos esta tarde?",
    "Â¿QuiÃ©n trae las partituras?",
    "Â¿Alguien tiene las baquetas de repuesto?",
    "Â¿A quÃ© hora es el bolo del sÃ¡bado?",
    "Â¿QuiÃ©n puede sustituirme en el ensayo?",
    "Â¿HabÃ©is visto el nuevo local de ensayo?",
    
    # Mezcla de ambos
    "Â¿Cerveza despuÃ©s del ensayo?",
    "Â¿Quedamos antes de tocar para tomar algo?",
    "Â¿Alguien se acuerda quÃ© tocamos ayer? ğŸ˜…",
    "Â¿QuiÃ©n viene al concierto? Primera ronda va por mi cuenta ğŸº",
    "Â¿Os acordÃ¡is de la Ãºltima vez que tocamos borrachos?",
    "Â¿Hacemos una jam session en el bar?",
    
    # Situaciones especÃ­ficas
    "Â¿QuiÃ©n se encarga de las bebidas para el local?",
    "Â¿Alguien grabÃ³ el ensayo de ayer? No me acuerdo de nada ğŸ¤£",
    "Â¿DÃ³nde dejÃ© mi instrumento anoche?",
    "Â¿QuiÃ©n conduce hoy? Yo quiero beber",
    "Â¿Repetimos lo del otro dÃ­a en el bar?",
    "Â¿Os acordÃ¡is de la Ãºltima vez que tocamos en ese garito?"
]

# Para cada miembro, generar respuestas a todos los prompts
for member in members:
    print(f"\nğŸ‘¤ Generando mensajes tÃ­picos de: {member}")
    print("=" * 50)
    
    try:
        # Obtener mensajes del miembro
        member_messages = df[df['sender'] == member]['message'].tolist()
        if not member_messages:
            print("âš ï¸ No se encontraron mensajes para este miembro")
            continue
            
        # Usar los Ãºltimos 10 mensajes o todos si hay menos
        sample_messages = member_messages[-10:] if len(member_messages) > 10 else member_messages
        
        # Generar respuesta para cada prompt
        for prompt in prompts:
            print(f"\nğŸ—¨ï¸ Prompt: {prompt}")
            
            # Crear el contexto con los mensajes de ejemplo
            context = f"""Analizando mensajes de {member}. Ejemplos:
{chr(10).join(f'- {msg}' for msg in sample_messages)}

BasÃ¡ndote en estos ejemplos, genera una respuesta tÃ­pica de {member} a: {prompt}
La respuesta debe mantener el mismo estilo, vocabulario y forma de escribir."""

            try:
                # Generar respuesta
                response = generator(
                    context,
                    max_length=100,
                    num_return_sequences=1,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=50256
                )
                
                # Extraer y limpiar la respuesta
                generated_text = response[0]['generated_text']
                final_response = generated_text.split(prompt)[-1].strip()
                if not final_response:
                    final_response = generated_text.split("respuesta")[-1].strip()
                
                print(f"ğŸ’­ Respuesta: {final_response}")
                
            except Exception as e:
                print(f"âŒ Error al generar respuesta: {str(e)}")
            
            print("-" * 30)
    
    except Exception as e:
        print(f"âŒ Error general al procesar miembro: {str(e)}")
    finally:
        print("=" * 50)

print("\nâœ… GeneraciÃ³n de mensajes tÃ­picos completada!")

# %% [markdown]
# ## ğŸ”„ Â¿Generar otro mensaje?

# %%
# Puedes ejecutar la celda anterior de nuevo con un prompt diferente
# O volver a la celda de selecciÃ³n de miembro para elegir otra persona 