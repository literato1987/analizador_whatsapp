# %% [markdown]
# # ü§ñ WhatsApp Chat Style Mimicker
# 
# Este script analiza un chat de WhatsApp y puede imitar c√≥mo habla cada persona.
# 
# ## üìù Antes de empezar:
# 1. Exporta un chat de WhatsApp (sin medios)
# 2. Pon el archivo .txt en esta carpeta
# 3. Instala las dependencias necesarias

# %% [markdown]
# ## üìö Importar lo necesario

# %%
import os
import re
import pandas as pd
from datetime import datetime
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import torch

print("üîß Configurando el modelo...")

# Usar un modelo m√°s ligero y gratuito
model_name = "facebook/blenderbot-400M-distill"
try:
    # Inicializar el generador de texto
    generator = pipeline(
        "text-generation",
        model=model_name,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    print("‚úÖ Modelo cargado correctamente")
except Exception as e:
    print(f"‚ùå Error al cargar el modelo: {str(e)}")
    print("Intentando descargar el modelo primero...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
        print("‚úÖ Modelo descargado y cargado correctamente")
    except Exception as e:
        raise Exception(f"‚ùå No se pudo cargar el modelo: {str(e)}")

# %% [markdown]
# ## üìÇ Buscar archivos de chat

# %%
# Ver qu√© archivos de chat hay disponibles
chat_files = [f for f in os.listdir() if f.endswith('.txt')]

print("üìÅ Archivos encontrados:")
for i, file in enumerate(chat_files, 1):
    print(f"{i}. {file}")

if not chat_files:
    print("‚ùå No se encontraron archivos .txt")
    raise Exception("Necesitas exportar un chat de WhatsApp primero")

# Elegir un archivo
file_number = int(input("\nElige el n√∫mero del archivo a analizar: "))
selected_file = chat_files[file_number - 1]
print(f"Archivo seleccionado: {selected_file}")

# %% [markdown]
# ## üìñ Leer y procesar el chat

# %%
# Leer el archivo y mostrar las primeras l√≠neas para debug
with open(selected_file, 'r', encoding='utf-8') as file:
    content = file.read()
    
print("Primeras 5 l√≠neas del archivo:")
print("\n".join(content.split('\n')[:5]))
print("\nTotal de l√≠neas:", len(content.split('\n')))

# Intentar diferentes patrones de regex para WhatsApp
patterns = [
    # Patr√≥n 1: Formato t√≠pico de WhatsApp
    r'(\d{1,2}/\d{1,2}/\d{2,4},\s\d{1,2}:\d{2})\s-\s([^:]+):\s(.+)',
    
    # Patr√≥n 2: Con AM/PM
    r'(\d{1,2}/\d{1,2}/\d{2,4},\s\d{1,2}:\d{2}\s[APMapm]{2})\s-\s([^:]+):\s(.+)',
    
    # Patr√≥n 3: Formato alternativo
    r'\[(\d{1,2}/\d{1,2}/\d{2,4}\s\d{1,2}:\d{2}:\d{2})\]\s([^:]+):\s(.+)'
]

matches = []
used_pattern = None

for pattern in patterns:
    matches = re.findall(pattern, content, re.MULTILINE)
    if matches:
        used_pattern = pattern
        print(f"\n‚úÖ Patr√≥n encontrado: {pattern}")
        print(f"Mensajes encontrados: {len(matches)}")
        break

if not matches:
    print("\n‚ùå No se pudo encontrar un patr√≥n v√°lido en el archivo")
    print("Por favor, verifica que el archivo es una exportaci√≥n de WhatsApp")
    raise Exception("Formato de archivo no reconocido")

# Crear DataFrame
df = pd.DataFrame(matches, columns=['datetime', 'sender', 'message'])
print("\nEstructura del DataFrame:")
print(df.head())

# Ver miembros del chat
members = df['sender'].unique().tolist()
print(f"\nüë• Miembros encontrados ({len(members)}):")
for i, member in enumerate(members, 1):
    print(f"{i}. {member}")

if not members:
    print("‚ùå No se encontraron miembros en el chat")
    raise Exception("No se pudieron extraer los miembros del chat")

# %% [markdown]
# ## üéØ Seleccionar miembro y analizar su estilo

# %%
# Elegir miembro
member_number = int(input("\nElige el n√∫mero del miembro a imitar: "))
selected_member = members[member_number - 1]

# Obtener mensajes del miembro
member_messages = df[df['sender'] == selected_member]['message'].tolist()
sample_messages = member_messages[-20:] if len(member_messages) > 20 else member_messages

print(f"\nAnalizando los √∫ltimos {len(sample_messages)} mensajes de {selected_member}")
print("\nEjemplos de mensajes:")
for i, msg in enumerate(sample_messages[:3], 1):
    print(f"{i}. {msg}")

# %% [markdown]
# ## üí¨ Generar mensaje en su estilo

# %%
# Preparar el prompt
prompt = input("\nEscribe el mensaje que quieres generar: ")

# Crear el contexto con los mensajes de ejemplo
context = "\n".join([
    "Ejemplos de c√≥mo habla esta persona:",
    *[f"- {msg}" for msg in sample_messages[-5:]],  # Usar los √∫ltimos 5 mensajes como ejemplo
    "\nAhora, responde en el mismo estilo a este mensaje:",
    prompt
])

try:
    # Generar respuesta
    response = generator(
        context,
        max_length=150,
        num_return_sequences=1,
        temperature=0.7,
        do_sample=True
    )

    # Extraer la respuesta generada
    generated_text = response[0]['generated_text']
    # Limpiar la respuesta (quedarnos solo con la √∫ltima parte despu√©s del prompt)
    final_response = generated_text.split(prompt)[-1].strip()

    print(f"\nüí¨ Mensaje generado en el estilo de {selected_member}:")
    print(final_response)
except Exception as e:
    print(f"‚ùå Error al generar el mensaje: {str(e)}")
    print("Intenta con un prompt m√°s corto o diferente")

# %% [markdown]
# ## üé≠ Generar mensaje t√≠pico de cada miembro

# %%
print("üîÑ Verificando el modelo...")

# Asegurarnos de que el modelo est√° cargado y que tenemos datos
if 'generator' not in locals():
    print("‚öôÔ∏è Cargando el modelo...")
    try:
        # Usar un modelo m√°s ligero y en espa√±ol
        model_name = "PlanTL-GOB-ES/gpt2-base-bne"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )
        print("‚úÖ Modelo cargado correctamente")
    except Exception as e:
        print(f"‚ùå Error al cargar el modelo: {str(e)}")
        print("Intentando un modelo alternativo...")
        try:
            # Intentar con un modelo m√°s peque√±o
            model_name = "bertin-project/bertin-gpt-j-6B-text"
            generator = pipeline(
                "text-generation",
                model=model_name,
                device="cuda" if torch.cuda.is_available() else "cpu"
            )
            print("‚úÖ Modelo alternativo cargado correctamente")
        except Exception as e:
            print(f"‚ùå Error al cargar el modelo alternativo: {str(e)}")
            raise Exception("No se pudo cargar ning√∫n modelo de generaci√≥n de texto")

# Verificar que tengamos datos de chat y miembros
if not members or df is None:
    print("‚ùå Error: No hay datos de chat o miembros cargados. Por favor, analiza un archivo de chat primero.")
    raise Exception("No hay datos de chat cargados")

print("\nüí¨ Generando mensajes t√≠picos de cada miembro...")

# Prompts variados para obtener diferentes tipos de mensajes
prompts = [
    # Relacionados con beber/fiesta
    "¬øAlguien se apunta a unas cervezas?",
    "¬øQuedamos en el bar de siempre?",
    "¬øQui√©n viene de cubatas?",
    "¬øHacemos una previa antes de salir?",
    "¬øAlguien tiene resaca hoy? ü§¢",
    "¬øD√≥nde nos tomamos la √∫ltima?",
    
    # Relacionados con m√∫sica/tocar
    "¬øEnsayamos esta tarde?",
    "¬øQui√©n trae las partituras?",
    "¬øAlguien tiene las baquetas de repuesto?",
    "¬øA qu√© hora es el bolo del s√°bado?",
    "¬øQui√©n puede sustituirme en el ensayo?",
    "¬øHab√©is visto el nuevo local de ensayo?",
    
    # Mezcla de ambos
    "¬øCerveza despu√©s del ensayo?",
    "¬øQuedamos antes de tocar para tomar algo?",
    "¬øAlguien se acuerda qu√© tocamos ayer? üòÖ",
    "¬øQui√©n viene al concierto? Primera ronda va por mi cuenta üç∫",
    "¬øOs acord√°is de la √∫ltima vez que tocamos borrachos?",
    "¬øHacemos una jam session en el bar?",
    
    # Situaciones espec√≠ficas
    "¬øQui√©n se encarga de las bebidas para el local?",
    "¬øAlguien grab√≥ el ensayo de ayer? No me acuerdo de nada ü§£",
    "¬øD√≥nde dej√© mi instrumento anoche?",
    "¬øQui√©n conduce hoy? Yo quiero beber",
    "¬øRepetimos lo del otro d√≠a en el bar?",
    "¬øOs acord√°is de la √∫ltima vez que tocamos en ese garito?"
]

# Para cada miembro, generar respuestas a todos los prompts
for member in members:
    print(f"\nüë§ Generando mensajes t√≠picos de: {member}")
    print("=" * 50)
    
    try:
        # Obtener mensajes del miembro
        member_messages = df[df['sender'] == member]['message'].tolist()
        if not member_messages:
            print("‚ö†Ô∏è No se encontraron mensajes para este miembro")
            continue
            
        # Usar los √∫ltimos 10 mensajes o todos si hay menos
        sample_messages = member_messages[-10:] if len(member_messages) > 10 else member_messages
        
        # Generar respuesta para cada prompt
        for prompt in prompts:
            print(f"\nüó®Ô∏è Prompt: {prompt}")
            
            # Crear el contexto con los mensajes de ejemplo
            context = f"""Analizando mensajes de {member}. Ejemplos:
{chr(10).join(f'- {msg}' for msg in sample_messages)}

Bas√°ndote en estos ejemplos, genera una respuesta t√≠pica de {member} a: {prompt}
La respuesta debe mantener el mismo estilo, vocabulario y forma de escribir."""

            try:
                # Generar respuesta
                response = generator(
                    context,
                    max_length=100,
                    num_return_sequences=1,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=50256
                )
                
                # Extraer y limpiar la respuesta
                generated_text = response[0]['generated_text']
                final_response = generated_text.split(prompt)[-1].strip()
                if not final_response:
                    final_response = generated_text.split("respuesta")[-1].strip()
                
                print(f"üí≠ Respuesta: {final_response}")
                
            except Exception as e:
                print(f"‚ùå Error al generar respuesta: {str(e)}")
            
            print("-" * 30)
    
    except Exception as e:
        print(f"‚ùå Error general al procesar miembro: {str(e)}")
    finally:
        print("=" * 50)

print("\n‚úÖ Generaci√≥n de mensajes t√≠picos completada!")

# %% [markdown]
# ## üîÑ ¬øGenerar otro mensaje?

# %%
# Puedes ejecutar la celda anterior de nuevo con un prompt diferente
# O volver a la celda de selecci√≥n de miembro para elegir otra persona 